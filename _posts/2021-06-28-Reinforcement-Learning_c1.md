---
layout: single
title:  "[강화학습] Introduction"
excerpt : "단단한 머신러닝 Chapter 1 Introduction"
summary: "Chapter 1 Introduction"

category : reinforcement-learning
tags: [강화학습]
toc : true
toc_sticky : true
toc_ads: true
use_math : true
---

## 들어가며  

이미 공부했었던 머신러닝/딥러닝과 관련된 내용들을 정리하고자 마음을 먹었었지만 새로운 것을 계속 배우면서 어느 정도 알고 있는 내용을 다시 공부해 정리한다는 것이 쉽지가 않음을 느껴 이번에 공부해보기로 마음먹은 강화 학습부터는 블로그에 정리를 하면서 공부를 하려고 합니다.  
정리하는 내용은 coursera에서 수강할 수 있는 [Reinforcement Learning Specialization](https://www.coursera.org/specializations/reinforcement-learning) 과정을 차례로 수강하며 Reinforcement Learning An Introduction 2판의 번역본인 단단한 강화학습의 내용을 정리할 예정입니다. 강의가 책의 내용에 기반해 준비되어있어 강의를 수강하고 책을 읽으면 공부를 시작하시는 분들도 도움이 많이 될 것 같습니다. 
포스팅은 책의 내용을 정리해 나중에 복습할 수 있을 정도로 정리하고, 특별한 툴이 필요하지 않은 책의 예제들은 수도코드를 참고해 파이썬으로 구현해 볼 예정입니다. 포기하지 않고 완독할 수 있길 바라며 시작합니다.

## Chapter 1 : Introduction

- 강화학습은 주어진 상황에서 어떤 행동을 취할지 학습하는 것을 의미
- 기계학습의 범주 중 
    - 지도 학습은 모델이 어떻게 행동해야 할 지에 대한 지침(Label)이 주어짐
    - 비지도 학습은 데이터 셋 내부의 숨겨진 구조를 학습하는 것을 의미함
- 강화학습은 지침(Label)이 주어지지 않고 데이터의 숨겨진 구조를 찾는 것도 아니고 오직 환경(environment)과 학습자(agent)의 상호작용으로 학습이 진행되기 때문에 앞의 두 개념과 구분됨

### 강화학습의 구성 요소

- 강화학습의 기본적인 구성 요소는 학습자와 환경이다.
- 둘을 제외하고는 정책(policy), 보상 신호(reward signal), 가치함수(value function)가 있음
- **정책**은 특정 시점에 학습자가 취하는 행동을 정의
    - 간단한 함수 형태
    - look up table(표)의 형태
    - 많은 연산을 필요로하는 알고리즘 등

- **보상 신호**는 최적화 과정에서 목적 함수와 같은 역할로 매 시간마다 학습자의 행동에 대해 부여되는 값.
    - 행동에 대한 *즉각적인* 평가
    - 일반적으로 보상 신호는 환경과 행동에 대한 확률론적 함수

- **가치 함수**는 장기적 관점에서 무엇이 좋은지에 관한 함수로 미래의 보상에 대한 예측을 포함
    - 가치 추정이 강화학습에서 핵심적 역할을 하고, 책의 전체에 걸쳐 가치 추정에 관한 내용을 설명함

- **모델**은 환경의 변화를 시뮬레이션 해 봄으로 현재 상태와 취해지는 행동에 따라 다음 상태와 보상을 예측할 수 있게 해주는 계획(planning)을 가능하게 해줌
